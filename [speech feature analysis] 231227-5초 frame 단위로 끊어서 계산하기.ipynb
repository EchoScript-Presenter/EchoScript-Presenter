{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffacf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "import aubio\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "!pip install aubio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4911cf",
   "metadata": {},
   "source": [
    "### Library 정리\n",
    "* AudioSegment는 Pydub library (음성파일조작)\n",
    "* AudioFile은 Speech_recognition library (for 음성인식)\n",
    "\n",
    "### 사람 음성의 평균 Pitch\n",
    "* 남성: 대략 85 Hz에서 180 Hz\n",
    "* 여성: 대략 165 Hz에서 255 Hz\n",
    "* 어린이: 대략 210 Hz에서 300 Hz \n",
    "\n",
    "### 일반적인 대화 소리 Volume\n",
    "* 60 ~ 70 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac06c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Information...\n",
      "Duration: 37.83700680272109 seconds\n",
      "Channels: 2\n",
      "Frame rate: 44100\n",
      "\n",
      "\n",
      "[start_time:0, end_time: 5000, total_time:37837]\n",
      "Segment_text.split(): ['what', 'can', 'I', 'do', 'for', 'you', 'have', 'a', 'seat', 'please']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 2.0\n",
      "Speech Rate (Syllables/Second): 3.0\n",
      "Mean Pitch: 14.43 Hz\n",
      "Volume (dB): -43.25 dB\n",
      "\n",
      "\n",
      "[start_time:5000, end_time: 10000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "[start_time:10000, end_time: 15000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "[start_time:15000, end_time: 20000, total_time:37837]\n",
      "Segment_text.split(): ['I', 'would', 'like', 'to', 'know', 'why', 'you', \"wouldn't\", 'meet']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.8\n",
      "Speech Rate (Syllables/Second): 2.6\n",
      "Mean Pitch: 29.34 Hz\n",
      "Volume (dB): -43.35 dB\n",
      "\n",
      "\n",
      "[start_time:20000, end_time: 25000, total_time:37837]\n",
      "Segment_text.split(): ['play', 'me', 'yesterday', \"I'm\", 'sorry']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.0\n",
      "Speech Rate (Syllables/Second): 1.4\n",
      "Mean Pitch: 14.18 Hz\n",
      "Volume (dB): -44.71 dB\n",
      "\n",
      "\n",
      "[start_time:25000, end_time: 30000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "[start_time:30000, end_time: 35000, total_time:37837]\n",
      "Segment_text.split(): ['I', 'believe', 'that', 'person', 'was', 'you']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.2\n",
      "Speech Rate (Syllables/Second): 2.2\n",
      "Mean Pitch: 13.91 Hz\n",
      "Volume (dB): -44.75 dB\n",
      "\n",
      "\n",
      "[start_time:35000, end_time: 37837, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "audio_file_path = './drama.wav'\n",
    "interval = 5000 # 10 seconds\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "audio = AudioSegment.from_file(audio_file_path)\n",
    "audio_duration = audio.duration_seconds\n",
    "print(\"Detailed Information...\")\n",
    "print(f\"Duration: {audio_duration} seconds\")\n",
    "print(f\"Channels: {audio.channels}\")\n",
    "print(f\"Frame rate: {audio.frame_rate}\\n\\n\")\n",
    "\n",
    "start_time = 0\n",
    "while start_time < len(audio):\n",
    "    \n",
    "    end_time = min((start_time + interval), len(audio))\n",
    "    print(f\"[start_time:{start_time}, end_time: {end_time}, total_time:{len(audio)}]\")\n",
    "    \n",
    "    segment = audio[start_time:end_time]\n",
    "    temp_file_path = \"./temp_audio.wav\"\n",
    "    segment.export(temp_file_path, format = \"wav\")\n",
    "    start_time = end_time\n",
    "    \n",
    "    try:\n",
    "        with sr.AudioFile(temp_file_path) as source:\n",
    "            segment_audio = recognizer.record(source)\n",
    "            segment_text = recognizer.recognize_google(segment_audio)\n",
    "\n",
    "        ###############################################################\n",
    "        # 단어 또는 음절 수(모음 수를 계산하여 근사치) 계산\n",
    "        words = len(segment_text.split())\n",
    "        print(\"Segment_text.split():\",segment_text.split())\n",
    "        syllables = sum(1 for char in segment_text if char.lower() in 'aeiou')\n",
    "\n",
    "        # 발화 속도 계산 (단어 당, 만약 분당 계산할 것이라면 * 60 해주기)\n",
    "        speech_rate_words_per_minute = round((words / segment.duration_seconds),2)\n",
    "        print(\"speech_segment_duration:\",segment.duration_seconds,\"seconds\")\n",
    "\n",
    "        # 발화 속도 계산 (음절 당)\n",
    "        speech_rate_syllables_per_minute = round((syllables / segment.duration_seconds),2)\n",
    "\n",
    "        print(f\"Speech Rate (Words/Second): {speech_rate_words_per_minute}\")\n",
    "        print(f\"Speech Rate (Syllables/Second): {speech_rate_syllables_per_minute}\")\n",
    "        \n",
    "        ###############################################################\n",
    "        # Pitch 및 Volume 계산\n",
    "        y, s = librosa.load(temp_file_path)\n",
    "\n",
    "        # Calculate pitch (fundamental frequency)\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=y, sr=s)\n",
    "        mean_pitch_per_frame = np.mean(pitches, axis=0)\n",
    "        pitch_mean = np.mean(mean_pitch_per_frame)\n",
    "        pitch_mean = Decimal(str(pitch_mean)).quantize(Decimal('0.00'), rounding=ROUND_HALF_UP)\n",
    "        print(f\"Mean Pitch: {pitch_mean} Hz\")\n",
    "        \n",
    "        ###############################################################\n",
    "\n",
    "        # Calculate volume (RMS energy) in decibels\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        volume = round(20 * np.log10(np.mean(rms)),2)\n",
    "        print(f\"Volume (dB): {volume} dB\\n\\n\")\n",
    "        \n",
    "        ###############################################################\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"No speech recognized in the current segment.\\n\\n\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error connecting to Google Speech Recognition service: {e}\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\\n\\n\")\n",
    "        \n",
    "    \n",
    "        \n",
    "print(\"Finished...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b23cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Information...\n",
      "Duration: 37.83700680272109 seconds\n",
      "Channels: 2\n",
      "Frame rate: 44100\n",
      "\n",
      "\n",
      "[start_time:0, end_time: 5000, total_time:37837]\n",
      "Segment_text.split(): ['what', 'can', 'I', 'do', 'for', 'you', 'have', 'a', 'seat', 'please']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 2.0\n",
      "Speech Rate (Syllables/Second): 3.0\n",
      "Mean Pitch: 276.78917486752465 Hz\n",
      "Volume (dB): 52.46381015579701 dB\n",
      "\n",
      "\n",
      "[start_time:5000, end_time: 10000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "[start_time:10000, end_time: 15000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "Too much break!\n",
      "\n",
      "\n",
      "[start_time:15000, end_time: 20000, total_time:37837]\n",
      "Segment_text.split(): ['I', 'would', 'like', 'to', 'know', 'why', 'you', \"wouldn't\", 'meet']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.8\n",
      "Speech Rate (Syllables/Second): 2.6\n",
      "Mean Pitch: 279.47322957198446 Hz\n",
      "Volume (dB): 50.747818952087336 dB\n",
      "\n",
      "\n",
      "[start_time:20000, end_time: 25000, total_time:37837]\n",
      "Segment_text.split(): ['play', 'me', 'yesterday', \"I'm\", 'sorry']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.0\n",
      "Speech Rate (Syllables/Second): 1.4\n",
      "Mean Pitch: 269.7111263736264 Hz\n",
      "Volume (dB): 50.63433815160225 dB\n",
      "\n",
      "\n",
      "[start_time:25000, end_time: 30000, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "[start_time:30000, end_time: 35000, total_time:37837]\n",
      "Segment_text.split(): ['I', 'believe', 'that', 'person', 'was', 'you']\n",
      "speech_segment_duration: 5.0 seconds\n",
      "Speech Rate (Words/Second): 1.2\n",
      "Speech Rate (Syllables/Second): 2.2\n",
      "Mean Pitch: 287.9020880361174 Hz\n",
      "Volume (dB): 51.20368314029123 dB\n",
      "\n",
      "\n",
      "[start_time:35000, end_time: 37837, total_time:37837]\n",
      "No speech recognized in the current segment.\n",
      "\n",
      "\n",
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "def remove_high_frequency_noise(signal, threshold): # 노이즈 많은 것 같아서 노이즈 리덕션 함수 정의\n",
    "    \n",
    "    fft_result = fft(signal) # 주파수 도메인으로 변경\n",
    "    frequencies = np.fft.fftfreq(len(fft_result), d=1/audio.frame_rate) # 주파수 크기 추출\n",
    "    magnitudes = np.abs(fft_result)\n",
    "\n",
    "    high_frequency_indices = np.where(frequencies > threshold) # 높은 스펙트럼 주파수 (노이즈 추정) 제거\n",
    "    magnitudes[high_frequency_indices] = 0\n",
    "    filtered_signal = ifft(magnitudes) # 시간 도메인으로 변경\n",
    "\n",
    "    return filtered_signal.real\n",
    "\n",
    "def count_fillers(text, filler_words):\n",
    "    \n",
    "    return sum(text.lower().count(word) for word in filler_words)\n",
    "\n",
    "def analyze_audio(audio_file_path, interval = 5000):\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    audio_duration = audio.duration_seconds\n",
    "    print(\"Detailed Information...\")\n",
    "    print(f\"Duration: {audio_duration} seconds\")\n",
    "    print(f\"Channels: {audio.channels}\")\n",
    "    print(f\"Frame rate: {audio.frame_rate}\\n\\n\")\n",
    "\n",
    "    start_time = 0\n",
    "    filler_words_count = 0\n",
    "    pause_count = 0\n",
    "    turn = 0\n",
    "    turn_list = []\n",
    "    while start_time < len(audio):\n",
    "        \n",
    "        turn += 1\n",
    "\n",
    "        end_time = min((start_time + interval), len(audio))\n",
    "        print(f\"[start_time:{start_time}, end_time: {end_time}, total_time:{len(audio)}]\")\n",
    "\n",
    "        segment = audio[start_time:end_time]\n",
    "        temp_file_path = \"./temp_audio.wav\"\n",
    "        segment.export(temp_file_path, format = \"wav\")\n",
    "        start_time = end_time\n",
    "                \n",
    "        try:\n",
    "                \n",
    "            with sr.AudioFile(temp_file_path) as source:\n",
    "                segment_audio = recognizer.record(source)\n",
    "                segment_text = recognizer.recognize_google(segment_audio)\n",
    "\n",
    "            \n",
    "            ###############################################################\n",
    "\n",
    "            # Filler words 계산\n",
    "            filler_words_count += count_fillers(segment_text, filler_words)\n",
    "            if (filler_words_count >= 5):\n",
    "                print(\"Too much filler words!\")\n",
    "                filler_words_count = 0\n",
    "\n",
    "            ###############################################################\n",
    "\n",
    "            # 단어 또는 음절 수(모음 수를 계산하여 근사치) 계산\n",
    "            words = len(segment_text.split())\n",
    "            print(\"Segment_text.split():\",segment_text.split())\n",
    "            syllables = sum(1 for char in segment_text if char.lower() in 'aeiou')\n",
    "\n",
    "            # 발화 속도 계산 (단어 당, 만약 분당 계산할 것이라면 * 60 해주기)\n",
    "            speech_rate_words_per_minute = round((words / segment.duration_seconds),2)\n",
    "            print(\"speech_segment_duration:\",segment.duration_seconds,\"seconds\")\n",
    "\n",
    "            # 발화 속도 계산 (음절 당)\n",
    "            speech_rate_syllables_per_minute = round((syllables / segment.duration_seconds),2)\n",
    "\n",
    "            print(f\"Speech Rate (Words/Second): {speech_rate_words_per_minute}\")\n",
    "            print(f\"Speech Rate (Syllables/Second): {speech_rate_syllables_per_minute}\")\n",
    "\n",
    "            ###############################################################\n",
    "\n",
    "            # Pitch 및 Volume 계산\n",
    "            audioPV = AudioSegment.from_file(temp_file_path)\n",
    "            audioPV_array = np.array(audioPV.get_array_of_samples())\n",
    "\n",
    "            audioPV_array = remove_high_frequency_noise(audioPV_array,2000)\n",
    "\n",
    "            fft_result = fft(audioPV_array)\n",
    "            frequencies = np.fft.fftfreq(len(fft_result), d=1/audioPV.frame_rate)\n",
    "            magnitudes = np.abs(fft_result)\n",
    "            peaks,_ = find_peaks(magnitudes, height = 5000)\n",
    "            pitch_values = frequencies[peaks]\n",
    "            valid_pitches = [p for p in pitch_values if 50 < p < 500]\n",
    "            average_pitch = np.mean(valid_pitches) if valid_pitches else None\n",
    "            print(f\"Mean Pitch: {average_pitch} Hz\")\n",
    "\n",
    "            ###############################################################\n",
    "\n",
    "            # Calculate volume (RMS energy) in decibels\n",
    "            rms = np.sqrt(np.mean(audioPV_array**2))\n",
    "            volume = 20 * np.log10(rms)\n",
    "            print(f\"Volume (dB): {volume} dB\\n\\n\")\n",
    "\n",
    "            ###############################################################\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            ###############################################################\n",
    "\n",
    "            # Pause 계산 ★★★ 다시 구하기 >> pyaudio할 때 ★★★\n",
    "            pause_count += 1\n",
    "            turn_list.append(turn)\n",
    "            if pause_count >= 2 and abs(turn_list[1]-turn_list[0]) == 1: \n",
    "                print(\"No speech recognized in the current segment.\")\n",
    "                print(\"Too much break!\\n\\n\")\n",
    "                pause_count = 0\n",
    "                turn_list = []\n",
    "            else:\n",
    "                print(\"No speech recognized in the current segment.\\n\\n\")\n",
    "\n",
    "            \n",
    "            ###############################################################\n",
    "\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error connecting to Google Speech Recognition service: {e}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Finished...\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filler_words = [\"Um\",\"Uh\",\"Like\",\"You know\",\"Well\"]\n",
    "    audio_file_path = './drama.wav'\n",
    "    analyze_audio(audio_file_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe8554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac89122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03885f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3660e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def detect_silence(audio_data, silence_threshold=-40):\n",
    "    # audio_data에서 음성 중단을 감지하고 해당 구간을 반환\n",
    "    is_silent = np.max(audio_data) < silence_threshold\n",
    "    return is_silent\n",
    "\n",
    "def monitor_microphone(input_device_index=None, silence_threshold=-40, break_duration_threshold=10):\n",
    "    CHUNK = 1024  # 오디오 스트림에서 한 번에 읽을 데이터 크기\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100  # 샘플링 레이트 (Hz)\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=input_device_index,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Monitoring microphone...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    is_breaking = False\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            data = np.frombuffer(stream.read(CHUNK), dtype=np.int16)\n",
    "            if detect_silence(data, silence_threshold=silence_threshold):\n",
    "                if not is_breaking:\n",
    "                    is_breaking = True\n",
    "                    start_time = time.time()\n",
    "            else:\n",
    "                is_breaking = False\n",
    "\n",
    "            if is_breaking and time.time() - start_time > break_duration_threshold:\n",
    "                print(f\"Too much break! ({break_duration_threshold} seconds)\")\n",
    "                is_breaking = False\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print(\"Monitoring stopped.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "# 마이크 입력 모니터링 시작\n",
    "monitor_microphone(input_device_index=None, silence_threshold=-40, break_duration_threshold=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
